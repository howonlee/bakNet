\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\newcommand{\del}{\nabla}
\begin{document}

\title{Psych209 Final Project}
\author{Howon Lee}
\maketitle
\section*{Abstract}
%%% let's have the abstract last
\section*{Introduction}
\subsection*{Issue Addressed}

We wish to address the issue of using an alternative approach to introducing necessary non-determinism to the constraint satisfaction problem in a PDP system, as opposed to the Boltzmann machine with conventional simulated annealing.

An alternative optimization algorithm called $\tau$-extremal optimization($\tau$-EO) is used to update a constraint-satisfaction framework to escape local minima in short amounts of time. It, like simulated annealing, is inspired by condensed matter physics, but $\tau$-EO deals with nonequilibrium systems.

\subsection*{Why is it interesting?}

The update rule for Boltzmann machines in the general, non-restricted case is very powerful but not usable in practice. This is in part because of the enormous time needed to approach the network's equilibrium distribution. Therefore, other ways of avoiding local maxima and introducing nondeterminism in constraint satisfaction, besides simulated annealing of Boltzmann machines, might be of interest. Simulated annealing also has the disadvantage that it must be given an annealing schedule to work effectively, the proper tuning of which is extremely hard and problem-specific. Another source of interest is the possible exploration of the semantics of alternative ways to introduce nondeterminism into the constraint satisfaction framework in order to avoid the problem of local optima.

\subsection*{What has previously been done?}
%%% cite: extremal optimization, genetic algorithms
Extremal optimization, in general, is inspired by models of co-evolution. This is as opposed to genetic algorithms and evolutionary algorithms, which are inspired by evolution in the abstract.

%%% cite: bak-sneppen, gould
To be specific, there exists a model of evolution called the Bak-Sneppen model, designed to bring about the time dynamics and other qualitative phenomena of evolution in the simplest method possible. It is a one-dimensional lattice model which models nothing about each species except its fitness and its relations with other species. $N$ species are considered as nodes on a graph which is a one-dimensional lattice (a ring), and each species has a fitness, denoted by a single scalar $\lambda$. At each discrete time step, the least fit species is replaced, and both of its neighbors replaced with it, with new species identical to the old ones except for the fitnesses, which are random. Notably, this selection organizes a fitness threshhold, below which species do not survive because they are always the least fit.

\begin{figure}
  \includegraphics{bak_sneppen}
  %%% let's have a nice caption telling us stuff, citing the stuff.
\end{figure}

%%% cite Bak and Chialvo
One unpromising avenue of exploration was the adaptive extremal dynamics architecture promulgated by Bak and Chialvo  %%%%%%%%%%%%%%%%%%%%%%%%% why?
%Bak and Chialvo's model, terrible model
  \begin{figure}
    \includegraphics{bak_chialvo_net_topology}
  \end{figure}
%Problems: Conjunctive neurons
%Comparison to simulated annealing
  \begin{figure}
    \includegraphics{bak_plot}
  \end{figure}

%%% cite Boetticher
The model creates punctuated equilibrium because as the smallest fitness increases, it becomes more likely that the next smallest fitness is adjacent to the previous smallest fitness, and therefore the events become correlated, causing "avalanches". This behavior, if one considers the nodes in the graph to be variables in a problem instead of species, was noted to be desirable in optimization problems by Boettcher and Percus, and used as the basis of extremal optimization.

Extremal optimization can be described succinctly as iteratively identifying the worst performing variable (according to the variable's fitness, which the implementer must define) in a given solution and replacing them with a new component or swapping them out with another component. Since this is trivially subject to local optima, this paper will mainly deal with a variant, $\tau$-extremal optimization, which solves this problem. It does so by ordering the local fitnesses and choosing the $k$'th one, where

$$P(k) \propto k^{-\tau}$$

%%% cite the analytic claim that there's an optimum value of tau.
Where $\tau$ is the one parameter. There is a claim by Boettcher et al that there is an analytically optimal value of $\tau$ for search time, which is $\tau = 1 + \frac{1}{\ln n}$, where $n$ is the number of variables.%%% by who??

%Existing results, from Boetticher: TSP, Ising model, talk about ising and hopfield net connection
%$\tau$-EO
  \begin{figure}
    \includegraphics{eo_alg}
  \end{figure}
%The hope:
  \begin{figure}
    \includegraphics{boettcher}
  \end{figure}
  
\section{Novel Approach}
  %Why not feedforward? BM vs. Ising Model

  %Locality: RBM

  \begin{figure}
    \includegraphics{rbm_eq}
  \end{figure}

%What approach will you be taking to address it? (at a big picture level; novel and/or otherwise noteworthy aspects)

\section{Details of Approach}

%Specific target phenomenon or phenomena you will be addressing: e.g., pattern of data you intend to try to fit. Model network task setting, architecture, processing and learning algorithm, training environment (corpus used for training, knowledge base built in to network),

%%% note the bak net try
%%% note the rbm try
%%% going to note the fact that I did bm normally now

\section{Results and Analysis}
  
  \frametitle{Ising Model}
  \begin{figure}
    \includegraphics{2000}
  \end{figure}
  \begin{figure}
    \includegraphics{10000}
  \end{figure}
  
  \frametitle{Ising Model}
  \begin{figure}
    \includegraphics{ising_energy_unzoomed}
  \end{figure}
  
  \begin{figure}
    \includegraphics{ising_energy_zoomed}
  \end{figure}

  See performance in RBM learning, as simple as possible

  Bit string: metaphor of Hamming cliff in GA

  Actually, this is just a weird coordinate descent
  
  \begin{figure}
    \includegraphics{eo_rbm_unzoomed}
  \end{figure}
 
  \begin{figure}
    \includegraphics{eo_rbm_zoomed}
  \end{figure}

  Make an algorithm that actually differs from CDiv

  Try non-restricted RBM, and learn with regular gradient

  (using EO in place of SA)


%%First present your primary findings that bear directly on the target phenomena.
%%% it's faster

%%A strong paper will, in addition, present an analysis of why the results came out the way they did, especially in cases where results did not come out as expected. 
%%% why is it faster?
%%% note the fasterness of SA compared to schedule SA is partly an artifact of the fact that we demand a global minima: the schedule tends to get stuck in local minima
%%% but why is the tauEO faster? immediately after it makes a huge jump to escape local minima, the power law distribution of tau means that it will tend to explore the new local state space really fast, for basically the same reason that it's an online algorithm.

%%It can be useful to discuss results you obtain with one of us to get suggestions as to how to fully understand your findings.

%%Analysis not only of network outputs but also of the structure of the information present in the materials you use to train your network (when relevant) and / or the hidden unit activations, network weights, or learning trajectory can help illuminate why and how your network has performed the way it did.

\section{Discussion}

It must be noted that, unlike simulated annealing with an annealing schedule, which doesn't work as an online algorithm, or an algorithm which can be given its input piece by piece, that there is nothing preventing $\tau$-extremal optimization from being used online.

%%% cite Boettcher for the disequilibrium stuff
The theoretical basis for the difference 

%%% theoretincal thoughts: out of equilibrium
%%% theoretical thoughts: black magic in determining the number of iterations, very associated with the fact of being an online algorithm

%Summarize your goals, your approach, and your main findings, and state a conclusion indicating how well, overall, your goals have been met.

%%% goals: make a faster thing than SA for abstract csp
%%% approach: tau-EO for cs net
%%% findings: spiffy results
%Then, discuss shortcomings or limitations of your effort and indicate how these might be overcome in future work.
%%% limitations: limited net, not fully connected, not a real problem. doesn't scale up.
%%% limitations: not thinking hard about neuronal connectivity. one thing that continues on the subject of criticality is modelling chialvo's work
%%% broader implications: try to look carefully at the critical regions of abstract NP-complete problem
%%Also, indicate broader implications and potential future applications of the ideas and approach.

\subsection*{What remains to be done?}
Because this project attempted to completely explore an entire state space of a problem, it was not feasible to explore a non-toy problem: that is, a problem where it's not possible to explore the entire state space, and therefore other measures of speech must be checked. It remains to be seen if this speed can be replicated in a larger problem domain with different local optima.

%%% cite chialvo
%%% compare to Chialvo stuff

\section{Summary}

Citations:   We will not be sticklers for details of citation styles but do provide citations to literature you draw on in your paper, using the following format

\end{document}

