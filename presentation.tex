\documentclass{beamer}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\begin{document}
\title{Extremal Optimization}
\author{Howon Lee}
\maketitle

\begin{frame}
  \frametitle{What is the question?}
  How can you use extremal dynamics to train some kind of neural net?
\end{frame}

\begin{frame}
  \frametitle{What? What's that?}
  Qualitative model of evolution: Bak-Sneppen model.
  \begin{figure}
    \includegraphics{bak_sneppen}
  \end{figure}

  Rules of Bak-Sneppen model. How does it behave?

  Caveats: CR Shalizi, W Tozier, "A Simple Model of the Evolution of Simple Models of Evolution"
\end{frame}

\begin{frame}
  \frametitle{Approaches Taken}
  Bak and Chialvo's model

  More abstract: extremal optimization

  Finally: Using $\tau$-EO on RBM
\end{frame}

\begin{frame}
  \frametitle{Adaptive Learning by Extremal Dynamics and Negative Feedback}
  How it works, from 20000 feet

  Problems: Conjunctive neurons %%% chart showing the conjunctive neurons
  %%% shit results on Bak net. note that the shit results are a little less worse for natural data

  Forgiveness?

  Similarity to Tabu search, simulated annealing?
  %%% have my replication, and my results?
\end{frame}

\begin{frame}
  \frametitle{More abstractly: EO, $\tau$-EO}
  How it works, from 20000 feet

  Existing results, from Boetticher: TSP
  %%% steal a chart
  %%% Talk about extremal optimization in the more abstract sense. Travelling salesman problem. Boetticher's work on the deterministic EO and tau-EO.
\end{frame}

\begin{frame}
  \frametitle{$\tau$-EO on neural nets}
  Why not feedforward?

  Locality: RBM

  %%% let's have a completely null graph here
\end{frame}


\begin{frame}
  \frametitle{Task}
  See performance in machine learning task

  %%details of approach. specific target phenomenon. model network task setting, architecture, processing and learning algorithm, training environment
  Locality of fitness much better with RBM.
  %%% Figuring out applying the abstract EO algorithm to things. Note that the locality of fitness that the model requires is much easier to do in Boltzmann machine, decide to do RBM on those machines.
\end{frame}

\begin{frame}
  \frametitle{Results on RBM}
  %%%% charts charts charts
  %%% Results. Try some classifications. Do time course of learning on the shit error measure that we have and a non-shit error measure. Compare to gradient descent for learning. Apologize for not having simulated annealing to compare to.
\end{frame}

\begin{frame}
  \frametitle{Problems, Issues}
  Becoming really gradient-free?

\end{frame}

\begin{frame}
  \frametitle{Going forward}
  %5In report: compare with simulated annealing? maybe?
  In report: look at created weights
  After class: use in a deep architecture
  After class: investigate with non-restricted BM's
  %%%% speculation speculation speculation
\end{frame}

\end{document}

